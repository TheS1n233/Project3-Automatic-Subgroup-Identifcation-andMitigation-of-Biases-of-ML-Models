lambda=0.6  seed=42


üîÅ  Starting Epoch 1/5
Batch   1/75  - Loss: 0.8299  - Acc: 0.4375
Batch  11/75  - Loss: 0.2512  - Acc: 0.8125
Batch  21/75  - Loss: 0.1412  - Acc: 0.8750
Batch  31/75  - Loss: 0.1406  - Acc: 0.9022
Batch  41/75  - Loss: 0.0606  - Acc: 0.9150
Batch  51/75  - Loss: 0.1483  - Acc: 0.9234
Batch  61/75  - Loss: 0.0945  - Acc: 0.9262
Batch  71/75  - Loss: 0.0407  - Acc: 0.9298
Batch  75/75  - Loss: 0.1490  - Acc: 0.9306
‚úÖ  Epoch 1 done. Loss: 0.1807, Acc: 0.9306

üîÅ  Starting Epoch 2/5
Batch   1/75  - Loss: 0.0539  - Acc: 0.9688
Batch  11/75  - Loss: 0.0569  - Acc: 0.9759
Batch  21/75  - Loss: 0.0330  - Acc: 0.9784
Batch  31/75  - Loss: 0.0298  - Acc: 0.9819
Batch  41/75  - Loss: 0.0605  - Acc: 0.9813
Batch  51/75  - Loss: 0.0302  - Acc: 0.9813
Batch  61/75  - Loss: 0.0451  - Acc: 0.9831
Batch  71/75  - Loss: 0.0997  - Acc: 0.9815
Batch  75/75  - Loss: 0.0370  - Acc: 0.9819
‚úÖ  Epoch 2 done. Loss: 0.0524, Acc: 0.9819

üîÅ  Starting Epoch 3/5
Batch   1/75  - Loss: 0.0164  - Acc: 1.0000
Batch  11/75  - Loss: 0.0215  - Acc: 0.9972
Batch  21/75  - Loss: 0.0158  - Acc: 0.9940
Batch  31/75  - Loss: 0.0119  - Acc: 0.9940
Batch  41/75  - Loss: 0.0264  - Acc: 0.9939
Batch  51/75  - Loss: 0.0394  - Acc: 0.9936
Batch  61/75  - Loss: 0.0260  - Acc: 0.9928
Batch  71/75  - Loss: 0.0091  - Acc: 0.9916
Batch  75/75  - Loss: 0.0081  - Acc: 0.9914
‚úÖ  Epoch 3 done. Loss: 0.0306, Acc: 0.9914

üîÅ  Starting Epoch 4/5
Batch   1/75  - Loss: 0.0061  - Acc: 1.0000
Batch  11/75  - Loss: 0.0163  - Acc: 0.9943
Batch  21/75  - Loss: 0.0038  - Acc: 0.9955
Batch  31/75  - Loss: 0.0110  - Acc: 0.9965
Batch  41/75  - Loss: 0.0163  - Acc: 0.9954
Batch  51/75  - Loss: 0.0054  - Acc: 0.9954
Batch  61/75  - Loss: 0.0117  - Acc: 0.9954
Batch  71/75  - Loss: 0.0208  - Acc: 0.9949
Batch  75/75  - Loss: 0.0108  - Acc: 0.9952
‚úÖ  Epoch 4 done. Loss: 0.0185, Acc: 0.9952

üîÅ  Starting Epoch 5/5
Batch   1/75  - Loss: 0.0019  - Acc: 1.0000
Batch  11/75  - Loss: 0.0086  - Acc: 0.9986
Batch  21/75  - Loss: 0.0047  - Acc: 0.9985
Batch  31/75  - Loss: 0.0066  - Acc: 0.9955
Batch  41/75  - Loss: 0.0096  - Acc: 0.9954
Batch  51/75  - Loss: 0.0079  - Acc: 0.9957
Batch  61/75  - Loss: 0.0056  - Acc: 0.9956
Batch  71/75  - Loss: 0.0039  - Acc: 0.9963
Batch  75/75  - Loss: 0.0264  - Acc: 0.9960
‚úÖ  Epoch 5 done. Loss: 0.0129, Acc: 0.9960
Baseline  overall 0.870  worst 0.522  gap 0.348

Divergence per group: {'0.0': 0.0, '1.0': 0.0, '2.0': 0.017857142857142856, '3.0': 0.005676442762535478}
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
üåà  DW Epoch 1  loss=0.1548  acc=0.942
üåà  DW Epoch 2  loss=0.0564  acc=0.981
üåà  DW Epoch 3  loss=0.0280  acc=0.993
üåà  DW Epoch 4  loss=0.0224  acc=0.994
üåà  DW Epoch 5  loss=0.0092  acc=0.999
Mitigation overall 0.831  worst 0.593  gap 0.238